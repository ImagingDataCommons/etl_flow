{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01190035",
   "metadata": {},
   "source": [
    "# IDC ETL, v10\n",
    "This notebook implements the IDC ETL process. It closely follows the steps in section 6 of [ETL Workflow, v10](https://docs.google.com/document/d/1luEnT0Vr5_VZwOYl2WaIwwHDvVfm0IBeXVzxzRGfzMs/edit#heading=h.2r0uhxc). Refer to that document for a description of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c854b2",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b562510",
   "metadata": {},
   "source": [
    "We do ETL development in Pycharm, wherein Pycharm is configured to execute ETL scripts on a remote VM. For purposes of remote execution, Pycharm maintains a copy of most of the files in a Pycharm project on the remote VM.\n",
    "Presumably, one could also pull/clone the project data from the [etl_flow](https://github.com/ImagingDataCommons/etl_flow) repo. We have not yet tried that.\n",
    "\n",
    "When performing the ingestion step, an 8-core VM is recommended to support multi-process downloading of the data. For other tasks, a 2-core VM is usually sufficient.\n",
    "\n",
    "Regardless, the following constants should be change to define the location of the etl_flow project.\n",
    "\n",
    "VM is an alias of the VM on which ETL scripts are executed. Such an alias can be generated by the [gcloud compute config-ssh](https://cloud.google.com/sdk/gcloud/reference/compute/config-ssh) CLI.\n",
    "\n",
    "EF is the top directory of the Pycharm etl_flow project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "VM = 'etl-dev-whc.us-central1-a.idc-etl-processing'\n",
    "EF = '/pycharm/etl_flow/tmp/pycharm_project_936'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca680ecb",
   "metadata": {},
   "source": [
    "Define an alias for initiating remote execution of an ETL script. It is used like:\n",
    "\n",
    "%remex utilities/tcia_helpers.py\n",
    "\n",
    "remex takes a single parameter that is the path from the project root to the script to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%alias remex ssh bcliffor@$VM env PYTHONPATH=/pycharm/etl_flow/tmp/pycharm_project_936:/pycharm/etl_flow/tmp/secure_files PYTHONUNBUFFERED=1 SECURE_LOCAL_PATH=../secure_files/etl SETTINGS_MODULE=settings python3.9 $EF/%s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a510bc",
   "metadata": {},
   "source": [
    "### A note on logging\n",
    "We are working to standardize logging as follows:\n",
    "\n",
    "Log files are created in the directory settings.LOGGING_BASE/settings.BASE_NAME where, currently:\n",
    "\n",
    "```\n",
    "LOGGING_BASE = f'/mnt/disks/idc-etl/logs/v{CURRENT_VERSION}'\n",
    "BASE_NAME = sys.argv[0].rsplit('/',1)[-1].rsplit('.',1)[0]\n",
    "LOG_DIR = f'{LOGGING_BASE}/{BASE_NAME}'\n",
    "```\n",
    "\n",
    "Then we configure three loggers, successlogger, errlogger and progresslogger, which output, respectively, to success.log, error.log and progress.log files in LOG_DIR. \n",
    "\n",
    "In general, outputs to success.log track completed operations, and are used to avoid repeating those operations in the event that execution of a script is interrupted and must be restarted. For example, a script that copies some set of instances from one bucket to another might log the name of each blob as it is successfully copied. Then, if that script must be restarted, it can input the contents of success.log, and skip copying any instance whose ID is in that input data.\n",
    "\n",
    "As its name implies, errors are logged to error.log.\n",
    "\n",
    "The progresslogger is generally used to log information about the progress of the operation. \n",
    "\n",
    "A particular script may use all, some or none of these loggers.\n",
    "\n",
    "Note that when a scripte is restarted, its the progress.log is truncated. The other log files are appended to.\n",
    "\n",
    "We often execute `tail -f xxx.log` on each of the log files in separate terminal windows to monitor progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75073b0",
   "metadata": {},
   "source": [
    "## Pre-ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528a44f",
   "metadata": {},
   "source": [
    "### Update settings.py\n",
    "\n",
    "The CURRENT_VERSION and PREVIOUS_VERSION in settings.py must be set as needed for the new version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5550a",
   "metadata": {},
   "source": [
    "### Import the DB\n",
    "Create a new database, typically \"idc_v\\<X\\>\", where X is the CURRENT_VERSION in settings. Then import from the saved final idc_v\\<Y\\> DB where Y is PREVIOUS_VERSION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585cdbe",
   "metadata": {},
   "source": [
    "### Revise the collection_id_map DB table\n",
    "Determine if there are any non-casing collectionID changes and revise collection_id as needed. For this purpose we execute the detect_tcia_collection_name_changes.py preingestion script. It uses the TCIA source DOI to map between TCIA collection IDs s and IDC collections names. Collection IDs that are only different in casing are ignored. Only failures are reported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a28931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex preingestion/detect_tcia_collection_name_changes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735d6f7",
   "metadata": {},
   "source": [
    "A failure indicates that the TCIA ID of a collection, which we refer to as the tcia_api_collection_id, has changed in more than just casing. If such a change is detected, the collection_id_map table must be manually updated. \n",
    "Note that this script only deals with TCIA collections. Dealing with other sources is TBD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5be51",
   "metadata": {},
   "source": [
    "### Update wsi metadata tables\n",
    "If there is new WSI data to be ingested, we must first update the WSI database tables. There are several parameters that likely need to be specified in order to override the defaults set in the script.\n",
    "We can get help to view from the build_wsi_metadata_tables_tsv.py script to view the possible parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1cfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex preingestion/wsi_build/build_wsi_metadata_tables_tsv.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab15138",
   "metadata": {},
   "source": [
    "As an example use of these parameters: At the time of writing, several HTAN collections have been copind into the *htan-transfer* bucket. The *HTAN-V1-Converted/Converted_20220416* folder in that bucket is the root of the specific conversion set which we want to ingest. The *identifiers.txt* blob in that folder is a tsv file that enumerates the instances in all the HTAN collections in the particular conversion. \n",
    "\n",
    "At this time, we only want to ingest the HTAN-OHSU collection and thus want to skip the HTAN-HMS, HTAN-Vanderbilt and HTAN-WUSTL collections. None of these collections are in collection groups, all of whose members are to be skipped.\n",
    "\n",
    "Therefore, we execute of build_wsi_metadata_tables_tsv.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf395b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%remex preingestion/wsi_build/build_wsi_metadata_tables_tsv.py \\\n",
    "    --src_bucket htan-transfer \\\n",
    "    --src_path HTAN-V1-Converted/Converted_20220416 \\\n",
    "    --tsv_blob identifiers.txt \\\n",
    "    --skipped_collections HTAN-HMS HTAN-Vanderbilt HTAN-WUSTL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81337152",
   "metadata": {},
   "source": [
    "Now verify that the new instances have been correctly added to the DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068abdc",
   "metadata": {},
   "source": [
    "As an aside, the preingestion/wsi_build/remove_wsi_metadata_tsv.py script does the opposite of build_wsi_metadata_tables_tsv.py: it removes any instances that is listed in a tsv and which it finds in the wsi_instance DB table. It also hierarchically removes emptied series, studies, patients and collections from the corresponding wsi tables. The parameterization is the sames as for build_wsi_metadata_tables_tsv.py. E.G., the following will remove the wsi metadata added above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex preingestion/wsi_build/remove_wsi_metadata_tsv.py \\\n",
    "    --src_bucket htan-transfer \\\n",
    "    --src_path HTAN-V1-Converted/Converted_20220416 \\\n",
    "    --tsv_blob identifiers.txt \\\n",
    "    --skipped_collections HTAN-HMS HTAN-Vanderbilt HTAN-WUSTL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1ca9e",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac896fea",
   "metadata": {},
   "source": [
    " We are now almost ready to ingest data. The base script is ingestion/ingest.py. It has several parameters that need to be configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex ingestion/ingest.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45073c52",
   "metadata": {},
   "source": [
    "ingestion/ingest.py can spawn multiple processes to speed up ingestion. If num_processes is 0, than all work is performed in the base process. We do not generally exceed 16 processes in order to avoid overloading the TCIA/NBIA server. \n",
    "\n",
    "Ingestion pulls radiology data from tcia, and pulls pathology data from a bucket or buckets into which it has been placed by the WSI conversion process and as specified in the wsi_instance table. Basically, ingestion asks each of the sources \"What collections do you have?\" and then proceeds to get new data and update our database. However, we often want to limit the collections which we try to get from one of the sources. In particular, we do not revise excluded collections; these are collections that we have previously ingested but then did not make public because they were considered of questionable quality. Similarly, we do not revise redacted collection; these are the collections that are known to contain head scans. So we specifically do not update the radiology component of these collections, but, in some cases do want to get pathology. We also do not ever try to update NLST radiology which is supposed never to change. However we have and will, for example, want to update NLST pathology.\n",
    "\n",
    "For this purpose, there are separate skipped_tcia_groups and skipped_path_groups parameters that separaetly list groups of collections which we don't want ingestion to process.\n",
    "\n",
    "The skipped_tcia_collections and skipped_path_collections parameters allow skipping additional collections from some other collection group.\n",
    "\n",
    "Collections identified by the include_tcia_collections and include_path_collections override corresponding sets of skipped collections. That is, the collections enumerated by these parameters are processed even if enumerated in one of the previous parameters.\n",
    "\n",
    "The server parameter, essentially, indicates whether to check for updates to the NLST collection. Because we don't expect to ever revise NLST, this parameter can be ignored.\n",
    "\n",
    "Ingestion copies data from the sources into per-version/per-collection/per-source buckets e.g. idc_v10_path_tcga_brca. The prestaging_tcia_bucket_prefix and prestaging_path_bucket_prefix parameters allows changing the bucket prefix from the default `idc_v<CURRENT_VERSION>_tcia_` and `idc_v<CURRENT_VERSION>_path_`\n",
    "\n",
    "Finally, it is often desirable to see which collections the ingestion process has identified as new, subject to revision, or about to be retired, before proceeding. If stop_after_collection_summary is True, ingestion will exit after printing out a summary of these pending changes, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex ingestion/ingest.py --stop_after_collection_summary True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5a148",
   "metadata": {},
   "source": [
    "In the following we have added two collections to both skipped_tcia_collection and skipped_path_collections. Notice that, because the collection IDs contain spaces, we must both escape the spaces and quote the IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ababd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex ingestion/ingest.py \\\n",
    "    --skipped_tcia_collections NLST HCC-TACE-Seg 'QIN\\ Breast\\ DCE-MRI' 'QIN\\ LUNG\\ CT' \\\n",
    "    --skipped_path_collections NLST HCC-TACE-Seg \"QIN\\ Breast\\ DCE-MRI\"  \"QIN\\ LUNG\\ CT\" \\\n",
    "    --stop_after_collection_summary True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3856c83",
   "metadata": {},
   "source": [
    "We can now perform ingestion. This can take several days, depending on the amount of data to be ingested. In particular, pulling data from NBIA generally runs at 8-12MB/s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex ingestion/ingest.py \\\n",
    "    --num_processes 12 \\\n",
    "    --skipped_tcia_collections NLST HCC-TACE-Seg 'QIN\\ Breast\\ DCE-MRI' 'QIN\\ LUNG\\ CT' \\\n",
    "    --skipped_path_collections NLST HCC-TACE-Seg \"QIN\\ Breast\\ DCE-MRI\"  \"QIN\\ LUNG\\ CT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27ab03",
   "metadata": {},
   "source": [
    "### Validate UUID uniqueness\n",
    "The DB will check for the uniqueness of UUIDs within a level, e.g. among all instances, or among all patiens, but not across levels. We now need to validate that there are no collisions among all IDC generated UUIDs. Such collisions are extremely unlikely but still possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex ingestion/validation/uuids_are_unique.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc49426",
   "metadata": {},
   "source": [
    "In the event that there is a collision, the UUID of the new instance, and the name of the corresponding blob must be changed. There is no script for this, so it must be done manually. Note that the `series_instance` many_to_many table is based on UUIDs, and so will also have to be changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2093b",
   "metadata": {},
   "source": [
    "### Other ingestion validation\n",
    "To a large extent, ingestion is self-validating. Specifically, the instance stage of ingestion verifies that after instances are copied to GCS, GCS has the expected blobs and that they each have the expected hash.\n",
    "\n",
    "Then, at each stage in the hierarchy, it compares hierarchical hashes with those from the corresponding source or sources. A hash mismatch prevents marking the corresponding objects as done. That, in turn, prevents marking higher level objects as done.\n",
    "\n",
    "Thus, if ingestion marks the new version as done, then all hierarchical hashes match the corresponding hashes of the sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2c061",
   "metadata": {},
   "source": [
    "## Post-ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa58da",
   "metadata": {},
   "source": [
    "### Revise collection group tables\n",
    "There are four collection group *tables* in the database:\n",
    "- cr_collections\n",
    "- defaced_collections\n",
    "- redacted_collections\n",
    "- excluded_collections\n",
    "\n",
    "These are described in the ETL Workflow document. \n",
    "Any collections not in these tables are defined as open collections. `open_collections` is a view that resolves to the metadata of all open collections.\n",
    "If ingestion adds a new collection to one of the cr, defaced, redacted or excludes collection groups, the corresponding table must be manually updated. This is expected to be a rare event so there is no script for this. Example SQL for manually updating the cr_collections table:\n",
    "```\n",
    "idc_v10=> insert into cr_collections values('ISPY2', 'cc608e04-d37b-4faa-86d8-5b17068c6f80', 'idc-dev-cr', 'idc-dev-cr', 'idc-open-cr', 'idc-open-cr', 'Public', 'Public')\n",
    "idc_v10=> insert into cr_collections values('ACRIN-6698', '50befbe5-9bd6-4d08-b766-b8825a8b7bb3', 'idc-dev-cr', 'idc-dev-cr', 'idc-open-cr', 'idc-open-cr', 'Public', 'Public');\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dc7c3",
   "metadata": {},
   "source": [
    "### Revise program table\n",
    "The program table associates each collection with a program. It is manually updated after updating a separately maintained [spreadsheet](https://docs.google.com/spreadsheets/d/1-sk8CMTDDj-deKv7sXglLvHUhDSNS1cRqUg5Oy5UpRY/edit#gid=0). Example SQL:\n",
    "\n",
    "```\n",
    "idc_v10=> insert into program values ('ISPY2', 'NCI Trials');\n",
    "idc_v10=> insert into program values ('ACRIN-6698', 'NCI Trials');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a3f22",
   "metadata": {},
   "source": [
    "### Populate a DICOM store and export metadata to dicom_metadata BQ table\n",
    "This step has several substeps.\n",
    "\n",
    "#### Import buckets all IDC data in version\n",
    "The first substep is to import all instances from the idc-dev-open, idc-dev-cr, idc-dev-defaced, and idc-dev retracted staging buckets, and the premerge buckets (e.g. idc_v9_path_tcga_gbm) into a new DICOM store. In other words we import into the DICOM store before merging the premerge buckets.\n",
    "\n",
    "We will eventually export DICOM metadata to BQ. For this purpose, we import from the redacted collections because we continue to include metadata of those collections in BQ. \n",
    "\n",
    "The default parameters do not normally need overriding.\n",
    "\n",
    "Note that the progress.log will show 'errors'. These are due to there being more one version of some instance in a bucket. However, a GCH DICOM store can only hold a single version of an instance (i.e an instance having a particular SOPInstanceUID) and  GCH reports, as an error, each attempt to import an instance having a SOPInstanceUID that is already in the DICOM store. We will deal with this issue in subsequent steps.\n",
    "\n",
    "We could avoid such errors import instance individually, but it is much more efficient to import entire buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/populate_dicom_store/step1_import_buckets_with_redacted.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ead335",
   "metadata": {},
   "source": [
    "#### Delete revised and retired instances from the DICOM store\n",
    "The errors described above occur when there is more than one version of an instance. When such an error has occurred, we know that one version of an instance was uploaded successfully, but any other versions were rejected. We need to end up with the most recent version of any such instance, but we have no way way of knowing which version of any instance was actually imported. \n",
    "\n",
    "Therefore, in this substep we delete from the DICOM store, all instances that have ever been revised. In addition, by importing the entire contents of a bucket, we will have imported any instances that have been \"retired\"...are no longer in the IDC version that we are building. So, in this substep, we also delete those retired instances.\n",
    "\n",
    "As above, the default parameters do not normally need overriding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7697ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/populate_dicom_store/step2_delete_revised_retired_instances_with_redacted.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9dac7",
   "metadata": {},
   "source": [
    "#### Insert revised instances\n",
    "Now that we have deleted all instances that have revisions, we load the latest revision of each such instance. We do not load instances that have been retired; they are not in this IDC version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/populate_dicom_store/step3_insert_revised_instances_with_redaction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbe31c",
   "metadata": {},
   "source": [
    "#### Export DICOM metadata \n",
    "The DICOM store now holds all instances whose metadata we want to export to BQ. The step4_export_metadata.py script exports DICOM metadata to the idc-dev-etl.idc_vX_pub.dicom_metadata table. There are no parameters that might need overriding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/populate_dicom_store/step4_export_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81fc9d",
   "metadata": {},
   "source": [
    "#### Delete redacted instances\n",
    "The DICOM store holds instance data from redacted collections. We now remove them from so that they cannot be viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/populate_dicom_store/step5_delete_redacted_instances.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81a43b",
   "metadata": {},
   "source": [
    "#### Validation  \n",
    "Validate that the dicom_metadata BQ table has the expected SOPInstanceUIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f70960",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gch/validate_dicom_store/validate_dicom_store_instance_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbc7ca",
   "metadata": {},
   "source": [
    "### Create an external connection\n",
    "Uploading DB tables to BQ, the next step, uses an external connection to access a Cloud SQL database. Each external connection is specific to a particular database, therefore we need to create an external connection for the DB of the new IDC version. Previously defined connections can be seen in the BQ console in the External connections dataset of idc-dev-etl project. \n",
    "\n",
    "The following script creates a new connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/bq_IO/utils/create_bq_external_connection.py \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072805f0",
   "metadata": {},
   "source": [
    "### Upload DB tables to BQ\n",
    "Next, we upload these DB tables to BQ. \n",
    "- analysis_id_map\n",
    "- collection_id_map\n",
    "- version\n",
    "- version_collection\n",
    "- collection\n",
    "- collection_patient\n",
    "- patient\n",
    "- patient_study\n",
    "- study\n",
    "- study_series\n",
    "- series\n",
    "- series_instance\n",
    "- instance\n",
    "- cr_collections\n",
    "- defaced_collections\n",
    "- excluded_collections\n",
    "- open_collections\n",
    "- redacted_collections\n",
    "- all_collections\n",
    "- all_included_collections\n",
    "- program\n",
    "- non_tcia_collection_metadata'\n",
    "\n",
    "All of these tables are uploaded by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94bb97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%remex bq/bq_IO/upload_psql_to_bq.vnext.dev.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad41e3",
   "metadata": {},
   "source": [
    "On occasion, we need to upload selected tables, e.g. if we have had to correct a table in the database. Use the --upload parameter for this, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/bq_IO/upload_psql_to_bq.vnext.dev.py \\\n",
    "    --upload 'analysis_results_descriptions'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208fe47",
   "metadata": {},
   "source": [
    "### Generate analysis_results_metadata\n",
    "Generate the analysis_results_metadata BQ table into the dev project. This script currently scrapes the TCIA analysis results page to get most of the data.\n",
    "The normal flow is to generate this table in the idc-dev-etl project and later copy to the PDP staging project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30779cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_analysis_results_table/gen_analysis_results_metadata_table.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183afd3",
   "metadata": {},
   "source": [
    "### Generate original_collections_metadata\n",
    "As for the analysis_results_metadata table, we'll copy to the PDP staging project later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_original_data_collections_table/gen_included_original_collection_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1d7a",
   "metadata": {},
   "source": [
    "### Generate excluded_collections_metadata\n",
    "excluded_collections_metadata includes the metadata of collections in the excluded_collections group. Unlike original_collections_metadata, it is not a public table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cadf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_original_data_collections_table/gen_excluded_original_collection_metadata.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75e76f",
   "metadata": {},
   "source": [
    "### Generate auxiliary_metadata\n",
    "The auxiliary_metadata BQ table defines the version. It contains IDC metadata for each instance in the version. In particular it includes the GCS URL of each instance.\n",
    "\n",
    "There are two GCS blobs for each instance. One instance is in one of several buckets in the idc-dev-etl project. Another copy is (will be after Google PDP staging) in Google PDP-owned or IDC-owned public buckets. (Actually there is only a single copy, in the idc-dev-excluded bucket) of instances in the excluded collections). Therefore we generate two versions of auxiliary_metadata: idc-dev-etl.idc_v\\<version\\>.auxiliary_metadata has GCS URLs in the idc-dev-etl buckets; idc-pdp-staging has GCS URLs in the public buckets. We generate tables separately.\n",
    "\n",
    "At this point in the process, the newly ingested data remains in the per-version/per-collection/per-source \"premerge\" buckets. The following script is configured to build from data in those buckets as well as from the idc-open, idc-dev-cr, idc-dev-defaced and idc-dev-redacted staging buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0679b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_aux_metadata_table/gen_auxiliary_metadata_table.dev.premerge.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cd6a3",
   "metadata": {},
   "source": [
    "Now we build the table in the idc-pdp-staging project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fabced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_aux_metadata_table/gen_auxiliary_metadata_table.pdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bfe90",
   "metadata": {},
   "source": [
    "Verify that auxiliary_metadata has the same set of SOPInstanceUIDs as dicom_metadata. We only check the dev version of auxiliary_metadata. First check the auxiliary_metadata in idc-dev-etl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_aux_metadata_table/validate/aux_matches_dicom_metadata.dev.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83f895",
   "metadata": {},
   "source": [
    "Then auxiliary_metadata in the PDP staging project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_aux_metadata_table/validate/aux_matches_dicom_metadata.pdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d570853",
   "metadata": {},
   "source": [
    " ### Generate version_metadata\n",
    " Create/update a table of per-version metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f71fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_version_metadata_table/gen_version_metadata_table/py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc45c8",
   "metadata": {},
   "source": [
    "### Copy bioclin tables\n",
    "\n",
    "At this time the TCGA and NLST bioclinical are in the idc-dev-etl.idc_v\\<version\\>\\_pub dataset rather than idc_clinical. They are generally unchanged across IDC versions so we just copy them from the previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7bfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/copy_tables/copy_bioclin_tables.vn-1_to_vn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef629434",
   "metadata": {},
   "source": [
    "### Copy BQ tables from dev to pdp datasets\n",
    "We can now copy the following tables from idc-dev-etl.idc_v\\<version\\>\\_pub to idc-pdp-staging.idc_v\\<version\\>:\n",
    "- analysis_results_metadata\n",
    "- dicom_metadata\n",
    "- nlst_canc\n",
    "- nlst_ctab\n",
    "- nlst_ctabc\n",
    "- nlst_prsn\n",
    "- nlst_screen\n",
    "- original_collections_metadata\n",
    "- tcga_biospecimen_rel9\n",
    "- tcga_clinical_rel9\n",
    "- version_metadata\n",
    "\n",
    "All the above are copied by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e128dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/copy_tables/copy_public_tables.dev_to_pdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00762c08",
   "metadata": {},
   "source": [
    "You can override the default --bqtables parameter, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/copy_tables/copy_public_tables.dev_to_pdp.py --bqtables nlst_canc nlst_ctab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e5ef6",
   "metadata": {},
   "source": [
    "### Generate open_collections_blob_names\n",
    "Generate a BQ table of the blob names (\\<uuid\\>.dcm) of all blobs that should be in the Google PDP public-datasets-idc bucket. This is for the use of Google PDP program and is only needed in the pdp project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_open_collections_blob_names/gen_open_collections_blob_names.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4b914",
   "metadata": {},
   "source": [
    "### Populate BQ views\n",
    "Several BQ views are now generated. First generate views in the idc-dev-etl project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex view_creation/BQ_Table_Building/publish_bq_views.dev.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca68183",
   "metadata": {},
   "source": [
    "Then generate views in the idc-pdp-staging project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13813fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/view_creation/BQ_Table_Building/publish_bq_views.pdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653f26a",
   "metadata": {},
   "source": [
    "Now validate that the views \"point\" to tables in the expected project/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8aeef0",
   "metadata": {},
   "source": [
    "### Generate idc_current dataset\n",
    "There is an `idc_current` dataset in both the idc-dev-etl and the idc-pdp-staging projects. For completeness, these must be created (or recreated) after the webapp team has generated the dicom_derived_all table and the dicom_pivot_v\\<version\\> in each of these projects. \n",
    "\n",
    "Generate the idc_current dataset in idc-dev-etl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc74bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_idc_current/gen_idc_current_dataset.dev.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1a091",
   "metadata": {},
   "source": [
    "and in idc-pdp-staging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_idc_current/gen_idc_current_dataset.pdp.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a1156",
   "metadata": {},
   "source": [
    "Now validate that the views \"point\" to tables in the expected project/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9755ac6",
   "metadata": {},
   "source": [
    "### Generate DCF manifest\n",
    "For each IDC version, we need DCF to index all new (versions of) instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex dcf/gen_instance_manifest/vX_instance_manifest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4916630",
   "metadata": {},
   "source": [
    "The script saves the resulting manifest in GCS (in the idc-dev-etl project) as gs://indexd_manifests/dcf_input/pdp_hosting/idc\\_v\\<version\\>\\_instance_manifest_\\*.tsv, where \\* is some number. The manifest for a version may be comprised of several parts. All such parts must be uploaded to [this](https://drive.google.com/drive/folders/1wNYfxLhX0Bhc_CCcuk_llB28YATX4aIm) DCF Drive folder for indexing. In addition, the new manifest name should be added to [this](https://docs.google.com/spreadsheets/d/1CcaPf4hjK9is2JbHnxpfmDYr9WJ7ZACI/edit#gid=820166513) spreadsheet.\n",
    "\n",
    "After DCF indexes a manifest, we do a statistical validation. dcf/validate_indexes/validate_indexes_in_version.py takes two parameters: --version specifies the version to be validated, --starts_with is a string of hex digits (0-9a-f). The script creates a list of UUIDs of new instances in --version which start with the --starts_with value. Thus, if --starts_with is '0', then all UUIDs that start with '0' are selected, resulting in a random selection of about one in sixteen instances. A --starts_with value of '00' selects one in 256, etc.\n",
    "\n",
    "The script then attempts to resolve each GUID at the DCF server and validate that the expected URL is returned. (It only validates instances (DRS blobs), not series or studies (DRS bundles).) Resolution is quite slow, about 3/sec, so --starts_with needs to be chosen so that validation completes in some reasonable time, (whatever 'reasonable' means to the validator.)\n",
    "\n",
    "\\-\\-version defaults to settings.CURRENT_VERSION  \n",
    "\\-\\-starts_with has no default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19e7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%remex dcf/validate_indexes/validate_indexes_in_version.py --version 9 --starts_with '000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a16430",
   "metadata": {},
   "source": [
    "### Populate the staging and public buckets\n",
    "We need to copy new instances to the staging and public buckets. \n",
    "Google PDP pulls new data from the idc-open-pdp-staging bucket. This is a “delta” bucket, containing only the instances that are new to a version. Therefore, it must be emptied before populating with new instances in the next version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/empty_bucket_mp/empty_pdp_bucket_mp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1aa277",
   "metadata": {},
   "source": [
    "Now copy dev staging buckets to PDP staging and IDC public buckets. The public buckets are idc-dev-cr and idc-dev-open1; these hold data from the cr, defaced, and redacted collection groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc26b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/copy_new_blobs_to pub_buckets/copy_new_blobs_to pub_buckets.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc08849",
   "metadata": {},
   "source": [
    "We need to validate that the staging and public buckets are correctly populated.\n",
    "Validate the staging bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/validate_bucket/validate_idc_open_pdp_staging.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fabdf4",
   "metadata": {},
   "source": [
    "Validate idc-open-cr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/validate_bucket/validate_idc_open_cr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ff239",
   "metadata": {},
   "source": [
    "Validate idc-open-idc1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be383c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/validate_bucket/validate_public_datasets_idc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc943494",
   "metadata": {},
   "source": [
    "## Post-Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76dc6c",
   "metadata": {},
   "source": [
    "### Validate PDP release\n",
    "Validate that the public-datasets-idc bucket has the correct set of instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e8d48",
   "metadata": {},
   "source": [
    "Check that the bigquery-public-datasets.idc_v\\<version\\> dataset is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/validate_bucket/validate_public_datasets_idc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467868c1",
   "metadata": {},
   "source": [
    "Check that the bigquery-public-datasets.idc_current dataset is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f597d5",
   "metadata": {},
   "source": [
    "### Merge the premerge buckets\n",
    "Up to this point, new data has remained in per-version/per-collection/per-source buckets in idc-dev-etl. We now move it to the staging buckets idc-dev-open, idc-dev-cr, idc-dev-defaced and idc-dev-redacted as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ded44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/copy_prestaging_to_staging/copy_prestaging_to_staging.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a71f75",
   "metadata": {},
   "source": [
    "Validate that the staging buckets contents are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145235b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex gcs/validate_bucket/validate_idc_dev_open.py\n",
    "%remex gcs/validate_bucket/validate_idc_dev_cr.py\n",
    "%remex gcs/validate_bucket/validate_idc_dev_defaced.py\n",
    "%remex gcs/validate_bucket/validate_idc_dev_redacted.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f9174",
   "metadata": {},
   "source": [
    "### Regenerate auxiliary_metadata\n",
    "Now that the premerge buckets have been merged, we need to regenerate the idc-dev-etl auxiliary_metadata table so that gcs_urls are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%remex bq/gen_aux_metadata_table/gen_auxiliary_metadata_table.dev.postmerge.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c737e6",
   "metadata": {},
   "source": [
    "### Delete premerge buckets\n",
    "The per-version/per-collection/per-source premerge buckets are now no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs debugging\n",
    "# %remex gcs/delete_prestaging_buckets/delete_prestaging_buckets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0460d",
   "metadata": {},
   "source": [
    "### Delete DICOM store\n",
    "Finally, delete the DICOM store for the previous IDC version. This is done via the GCH portal, e.g. [https://console.cloud.google.com/healthcare/browser?authuser=1&project=canceridc-data](https://console.cloud.google.com/healthcare/browser?authuser=1&project=canceridc-data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
